{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5882e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import glob\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear,Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as func\n",
    "import time\n",
    "from math import isnan,isinf,log,exp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c8b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname=r'../myp/tensors.p'\n",
    "rname='CIFAR10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82285964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../myp/tensors.p 15625\n"
     ]
    }
   ],
   "source": [
    "runs_cifar10=[]\n",
    "f = open(fname,'rb')\n",
    "while(1):\n",
    "    try:\n",
    "        runs_cifar10.append(pickle.load(f))\n",
    "    except EOFError:\n",
    "        break\n",
    "f.close()\n",
    "print(fname, len(runs_cifar10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f0c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2908\\3906831163.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  x_data_cifar10=torch.tensor([item.cpu().detach().numpy() for item in x_data_cifar10],dtype=torch.float64).cuda()\n"
     ]
    }
   ],
   "source": [
    "x_data_cifar10=[]\n",
    "y_data_cifar10=[]\n",
    "for idx in range(len(runs_cifar10)):\n",
    "    temp=runs_cifar10[idx]['logmeasures']['synflow']\n",
    "    for i in range(len(temp)):\n",
    "        temp[i]=temp[i].item()\n",
    "    temp=torch.tensor(temp,dtype=torch.float64)\n",
    "    x_data_cifar10.append(temp)\n",
    "    y_data_cifar10.append(runs_cifar10[idx]['testacc'])\n",
    "x_data_cifar10=torch.tensor([item.cpu().detach().numpy() for item in x_data_cifar10],dtype=torch.float64).cuda()\n",
    "y_data_cifar10=torch.tensor(y_data_cifar10,dtype=torch.float64)\n",
    "y_data_cifar10=y_data_cifar10/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b01423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625 15625\n"
     ]
    }
   ],
   "source": [
    "trainx_cifar10=[]\n",
    "trainy_cifar10=[]\n",
    "for i in range (len(x_data_cifar10)):\n",
    "    if isinf(torch.sum(x_data_cifar10[i]).item()):\n",
    "        continue\n",
    "    trainx_cifar10.append(x_data_cifar10[i])\n",
    "    trainy_cifar10.append(y_data_cifar10[i])\n",
    "trainx_cifar10=torch.tensor([item.cpu().detach().numpy() for item in trainx_cifar10],dtype=torch.float64)  \n",
    "trainy_cifar10=torch.tensor(trainy_cifar10,dtype=torch.float64)\n",
    "trainy_cifar10=torch.reshape(trainy_cifar10,(len(trainy_cifar10),1))\n",
    "print(len(trainx_cifar10),len(trainy_cifar10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054c59ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 3125\n"
     ]
    }
   ],
   "source": [
    "split_rate=0.2\n",
    "temp=int(len(trainx_cifar10)*(1-split_rate))\n",
    "testx_cifar10=trainx_cifar10[temp:]\n",
    "trainx_cifar10=trainx_cifar10[:temp]\n",
    "testy_cifar10=trainy_cifar10[temp:]\n",
    "trainy_cifar10=trainy_cifar10[:temp]\n",
    "print(len(trainx_cifar10),len(testx_cifar10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c093fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_cifar10=trainx_cifar10.cuda()\n",
    "trainy_cifar10=trainy_cifar10.cuda()\n",
    "testx_cifar10=testx_cifar10.cuda()\n",
    "testy_cifar10=testy_cifar10.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c125e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self,trainx,trainy):\n",
    "        self.trainx=trainx\n",
    "        self.trainy=trainy\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.trainx[idx],self.trainy[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trainx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a91e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=MyData(trainx_cifar10,trainy_cifar10)\n",
    "# test_dataset=MyData(testx_cifar10,testy_cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d33e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这句话很关键，nn默认类型为float32\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abd712ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数\n",
    "class SpearmanLossFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpearmanLossFunc,self).__init__()\n",
    "\n",
    "    def forward(self, t1,t2):\n",
    "        t1=t1.cpu().detach().numpy()\n",
    "        t2=t2.cpu().detach().numpy()\n",
    "        loss = -abs(stats.spearmanr(t1,t2,nan_policy='omit').correlation)\n",
    "        loss=torch.tensor(loss,requires_grad=True)\n",
    "        return loss\n",
    "    \n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss,self).__init__()\n",
    "\n",
    "    def forward(self, t1,t2):\n",
    "        t1=t1.cpu().detach().numpy()\n",
    "        t2=t2.cpu().detach().numpy()\n",
    "        loss=0\n",
    "#         t1=t1.reshape(-1,)\n",
    "#         t2=t2.reshape(-1,)\n",
    "        for i in range(len(t1)):\n",
    "            for j in range(len(t2)):\n",
    "                if i==j:\n",
    "                    continue\n",
    "                loss+=log(1+exp(-np.sign((t1[i]-t1[j])*(t2[i]-t2[j]))))\n",
    "        siz=len(t1)*(len(t1)-1)\n",
    "        loss=loss/siz\n",
    "        print(loss)\n",
    "        loss=torch.tensor(loss,dtype=torch.float64,requires_grad=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ad26d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3553, 0.8836], requires_grad=True) tensor([0.3854, 0.0936], requires_grad=True)\n",
      "1.3132616875182228\n",
      "tensor(1.3133, dtype=torch.float64, requires_grad=True)\n",
      "tensor([0.3553, 0.8836], requires_grad=True) tensor([0.3854, 0.0936], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 存在sign函数 无法求偏导，导致梯度均为None；反映在模型中，无法对output求偏导，导致output的梯度为None，从而无法反向传播\n",
    "loss_fn=MyLoss()\n",
    "t1 = torch.rand((2,), requires_grad=True)\n",
    "t2 = torch.rand((2,), requires_grad=True)\n",
    "print(t1,t2)\n",
    "loss = loss_fn(t1,t2)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(t1,t2)\n",
    "print(t1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7448767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class zyt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(zyt,self).__init__()\n",
    "        self.model1=Sequential(\n",
    "            nn.LayerNorm(188),\n",
    "            Linear(188,128),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            Linear(128,64),\n",
    "#             nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            Linear(64,10),\n",
    "            nn.ReLU(),\n",
    "            Linear(10,1),             \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.model1(x)\n",
    "        return x\n",
    "zyt1=zyt()\n",
    "zyt1=zyt1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88877252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn=torch.nn.MSELoss()#SpearmanLossFunc()\n",
    "loss_fn=MyLoss()\n",
    "loss_fn=loss_fn.cuda()\n",
    "#优化器\n",
    "learning_rate=0.01\n",
    "optimizer=torch.optim.Adam(zyt1.parameters(),lr=learning_rate)\n",
    "#训练的轮数\n",
    "epoch=100\n",
    "#batch\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c28805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=DataLoader(train_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2903c621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1010c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------第1轮训练开始-------\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(zyt1\u001b[38;5;241m.\u001b[39mmodel1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m     11\u001b[0m train_data,target\u001b[38;5;241m=\u001b[39mdata\n\u001b[1;32m---> 12\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mzyt1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m epoch_output\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_fn(output,target)\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36mzyt.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 19\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxspear=0\n",
    "for i in range(epoch):\n",
    "    epoch_output=[]\n",
    "    epoch_loss=0\n",
    "    zyt1.train()\n",
    "    print(\"------第{}轮训练开始-------\".format(i+1))\n",
    "#     start_time=time.time()\n",
    "    total_train_loss=0\n",
    "    for data in train_dataloader:\n",
    "        print(zyt1.model1[0].weight.grad)\n",
    "        train_data,target=data\n",
    "        output=zyt1(train_data)\n",
    "        epoch_output.append(output)\n",
    "        loss=loss_fn(output,target)\n",
    "        epoch_loss+=loss\n",
    "        #优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    epoch_output=[j.item() for i in epoch_output for j in i]\n",
    "    curspear=abs(stats.spearmanr(epoch_output,trainy_cifar10.cpu().detach().numpy(),nan_policy='omit').correlation)\n",
    "    print(curspear)\n",
    "    if (curspear>maxspear):\n",
    "        maxspear=curspear\n",
    "        maxepoch=i\n",
    "    print(\"train_loss:{}\".format(epoch_loss.item()))\n",
    "#测试步骤开始\n",
    "zyt1.eval() #让网络进入测试状态\n",
    "#no_grad()保证不影响梯度，不会进行调优\n",
    "with torch.no_grad():\n",
    "    test_output=zyt1(testx_cifar10)\n",
    "    test_loss=loss_fn(test_output,testy_cifar10)\n",
    "    print(\"test_loss:{}\".format(test_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944df7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
